{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP is a subfield of AI that focuses on understanding and processing human language.\n",
    "Steps:\n",
    "    Data Acquisition\n",
    "        Available: Table, Database, Less data\n",
    "        Other Source: Public Datasets, Web Scraping, API, pdf, image, audio\n",
    "        No Data\n",
    "    Text Preparation\n",
    "        Text Cleanup: HTML tags, emojis, spelling check\n",
    "        Basic Preprocessing\n",
    "            Basic: Tokenization(sentence & words)\n",
    "            Optional: stop words removal, stemming, removing punctuation, lowercasing, language detection\n",
    "        Advanced Preprocessing\n",
    "            POS: Parts of Speech Tagging\n",
    "            Parsing\n",
    "    Feature Engineering: Extracting input columns from text or converting text into numbers for ML model.\n",
    "        N-grams\n",
    "        TF-IDF\n",
    "        BoW: Bag of Words\n",
    "    Modeling\n",
    "        Model Building\n",
    "            Heuristic\n",
    "            ML approach\n",
    "            DL approach\n",
    "            Cloud API\n",
    "        Evaluation\n",
    "            Intrinsic\n",
    "            Extrinsic\n",
    "    Deployment\n",
    "        Deployment\n",
    "            API (microservice)\n",
    "            Chatbot\n",
    "        Monitoring\n",
    "        Model Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"IMDB dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lower Casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"review\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"review\"][3].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"review\"] = df[\"review\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"review\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Removing HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Sample HTML Document</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Welcome to our website!</h1>\n",
    "    <p>This is a sample HTML document with some <a href=\"https://example.com\">links</a> and <strong>formatted</strong> text.</p>\n",
    "    <ul>\n",
    "        <li>Item 1</li>\n",
    "        <li>Item 2</li>\n",
    "        <li>Item 3</li>\n",
    "    </ul>\n",
    "    <p>Thank you for visiting!</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    pattern = re.compile(\"<.*?>\")\n",
    "    return pattern.sub(r\"\", text)\n",
    "\n",
    "\n",
    "remove_html(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(remove_html(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"review\"].apply(remove_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Removing URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "1. https://www.example.com/page?param1=value1&param2=value2#section\n",
    "2. https://www.example.com/path/with/special%20characters/index.html\n",
    "3. http://www.test-site.org/page?param=1&param=2&param=3\n",
    "5. https://www.example.com/path/to/https://malicious-site.com/phishing\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    pattern = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    return pattern.sub(r\"\", text)\n",
    "\n",
    "\n",
    "remove_url(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(remove_url(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"review\"].apply(remove_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    for char in exclude:\n",
    "        text = text.replace(char, \"\")\n",
    "    return text\n",
    "\n",
    "\n",
    "df[\"review\"].apply(remove_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc1(text):  # 18 times faster than remove_punc\n",
    "    return text.translate(str.maketrans(\"\", \"\", exclude))\n",
    "\n",
    "\n",
    "df[\"review\"].apply(remove_punc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_text = \"My name is Krishana and I stydy in SRM\"\n",
    "text = TextBlob(incorrect_text)\n",
    "text.correct().string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Removing Stop Words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word in stopwords:\n",
    "            new_text.append(\"\")\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopwords(\"The quick brown fox jumps over the lazy dog.\")\n",
    "df[\"review\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Handling Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"\n",
    "        \"\\U0001F300-\\U0001F5FF\"\n",
    "        \"\\U0001F680-\\U0001F6FF\"\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"\n",
    "        \"\\U00002702-\\U000027B0\"\n",
    "        \"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE,\n",
    "    )\n",
    "    return emoji_pattern.sub(r\"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_emoji(\"HiðŸ˜Œ, I am KrishnaðŸ˜„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "print(emoji.demojize(\"Hi ðŸ˜Œ , I am Krishna ðŸ˜„\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Tokenization: Splitting a text into smaller units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #   Using the split function\n",
    "\n",
    "text1 = \"My name is Krishna\"\n",
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"Hi, My name is Krishna ðŸ˜Œ\"\n",
    "text2.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular Expression\n",
    "\n",
    "import re\n",
    "\n",
    "text = \"Hi, My name is Krishna ðŸ˜Œ\"\n",
    "tokens = re.findall(\"[\\w]+\", text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hi, my name is Krishna! and I have a pdf in A.I\"\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"SpaCy is an open-source natural language processing library that provides pre-trained models for various languages. It can tokenize text, perform part-of-speech tagging, named entity recognition, and much more. SpaCy's tokenization is fast and accurate, making it a popular choice for NLP tasks.\"\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"1. Running 2. Flies 3. Jumps 4. Sleeping 5. Better 6. Best 7. Swimmed 8. Swam 9. Feet 10. Children\"\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. BagOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"text\": [\n",
    "            \"people watch campusx\",\n",
    "            \"campusx watch campusx\",\n",
    "            \"people write comment\",\n",
    "            \"campusx write comment\",\n",
    "        ],\n",
    "        \"output\": [1, 1, 0, 0],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = cv.fit_transform(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[0], \" : \", bow[0].toarray())\n",
    "print(text[1], \" : \", bow[1].toarray())\n",
    "print(text[2], \" : \", bow[2].toarray())\n",
    "print(text[3], \" : \", bow[3].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.transform([\"campusx watch and write comment of campusx\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Bag of N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 3))\n",
    "bow = cv.fit_transform(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[0], \" : \", bow[0].toarray())\n",
    "print(text[1], \" : \", bow[1].toarray())\n",
    "print(text[2], \" : \", bow[2].toarray())\n",
    "print(text[3], \" : \", bow[3].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.transform([\"campusx watch and write comment of campusx\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF = tfidf.fit_transform(df[\"text\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[0], \" : \", TFIDF[0])\n",
    "print(text[1], \" : \", TFIDF[1])\n",
    "print(text[2], \" : \", TFIDF[2])\n",
    "print(text[3], \" : \", TFIDF[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.transform([\"campusx watch and write comment of campusx\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "story = []\n",
    "f = open(os.path.join(\"GOT.txt\"))\n",
    "text = f.read()\n",
    "raw_sent = sent_tokenize(text)\n",
    "for sent in raw_sent:\n",
    "    story.append(simple_preprocess(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(window=10, min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(story, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"daenerys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.doesnt_match([\"jon\", \"rikon\", \"robb\", \"arya\", \"sansa\", \"bran\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv[\"sansa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"arya\", \"sansa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"tywin\", \"sansa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using BOW and n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "df = temp_df.iloc[:10000]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "df.drop_duplicates(inplace=True)\n",
    "df[\"review\"] = df[\"review\"].apply(lambda x: x.lower())\n",
    "df[\"review\"].apply(remove_html)\n",
    "df[\"review\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 0:1]  # Seperating x & y\n",
    "y = df[\"sentiment\"]  # Seperating x & y\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)  # Replacing Positive&Negative with 1 & 0 in y\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applynig BoW\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X_train_bow = cv.fit_transform(X_train[\"review\"]).toarray()\n",
    "X_test_bow = cv.transform(X_test[\"review\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using ML Algorithm\n",
    "\n",
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_bow, y_train)\n",
    "y_pred = gnb.predict(X_test_bow)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "print(\"NaiveBayes AccuracyScore\", accuracy_score(y_test, y_pred))\n",
    "print(\"NaiveBayes ConfusionMatrix \\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_bow, y_train)\n",
    "y_pred = rf.predict(X_test_bow)\n",
    "print(\"RandomForest AccuracyScore\", accuracy_score(y_test, y_pred))\n",
    "print(\"RandomForest ConfusionMatrix\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Using Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=50000)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train[\"review\"])\n",
    "X_test_tfidf = tfidf.transform(X_test[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using ML Algo\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_tfidf, y_train)\n",
    "y_pred = rf.predict(X_test_tfidf)\n",
    "print(\"RandomForest AccuracyScore\", accuracy_score(y_test, y_pred))\n",
    "print(\"RandomForest ConfusionMatrix\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Using Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc1 = nlp(\"I will google about facebook\")\n",
    "doc2 = nlp(\"I left the room\")\n",
    "doc3 = nlp(\"I read books on history\")\n",
    "\n",
    "for word in doc1:\n",
    "    print(word.text, \"----->\", word.pos_, word.tag_, spacy.explain(word.tag_))\n",
    "print()\n",
    "for word in doc2:\n",
    "    print(word.text, \"----->\", word.pos_, word.tag_, spacy.explain(word.tag_))\n",
    "print()\n",
    "for word in doc3:\n",
    "    print(word.text, \"----->\", word.pos_, word.tag_, spacy.explain(word.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(\"The quick brown fox jumped over the lazy dog\")\n",
    "displacy.render(doc, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(cv, 'count_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(rf, 'random_forest_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import joblib\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "def SentimentAnalysis():\n",
    "    df = pd.read_csv(\"IMDB Dataset.csv\", nrows=10000)\n",
    "\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df[\"review\"] = df[\"review\"].apply(clean_text)\n",
    "\n",
    "    X = df[\"review\"]\n",
    "    y = df[\"sentiment\"]\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    y = encoder.fit_transform(y)\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_features=5000)\n",
    "    X_tfidf = tfidf.fit_transform(X)\n",
    "\n",
    "    rf = RandomForestClassifier()\n",
    "\n",
    "    cv_scores = cross_val_score(rf, X_tfidf, y, cv=10, scoring='accuracy')\n",
    "\n",
    "    print(\"Cross-Validation Scores:\", cv_scores)\n",
    "    print(\"Mean Accuracy:\", cv_scores.mean())\n",
    "\n",
    "    rf.fit(X_tfidf, y)\n",
    "\n",
    "    joblib.dump(tfidf, 'TfidfVectorizer.pkl')\n",
    "    joblib.dump(rf, 'random_forest_model.pkl')\n",
    "\n",
    "SentimentAnalysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
